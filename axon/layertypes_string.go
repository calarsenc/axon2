// Code generated by "stringer -type=LayerTypes"; DO NOT EDIT.

package axon

import (
	"errors"
	"strconv"
)

var _ = errors.New("dummy error")

func _() {
	// An "invalid array index" compiler error signifies that the constant values have changed.
	// Re-run the stringer command to generate them again.
	var x [1]struct{}
	_ = x[SuperLayer-0]
	_ = x[InputLayer-1]
	_ = x[TargetLayer-2]
	_ = x[CompareLayer-3]
	_ = x[CTLayer-4]
	_ = x[PulvinarLayer-5]
	_ = x[TRNLayer-6]
	_ = x[PTMaintLayer-7]
	_ = x[PTPredLayer-8]
	_ = x[PTNotMaintLayer-9]
	_ = x[MatrixLayer-10]
	_ = x[STNLayer-11]
	_ = x[GPLayer-12]
	_ = x[BGThalLayer-13]
	_ = x[VSGatedLayer-14]
	_ = x[BLALayer-15]
	_ = x[CeMLayer-16]
	_ = x[VSPatchLayer-17]
	_ = x[LHbLayer-18]
	_ = x[DrivesLayer-19]
	_ = x[EffortLayer-20]
	_ = x[UrgencyLayer-21]
	_ = x[USLayer-22]
	_ = x[PVLayer-23]
	_ = x[LDTLayer-24]
	_ = x[VTALayer-25]
	_ = x[RewLayer-26]
	_ = x[RWPredLayer-27]
	_ = x[RWDaLayer-28]
	_ = x[TDPredLayer-29]
	_ = x[TDIntegLayer-30]
	_ = x[TDDaLayer-31]
	_ = x[LayerTypesN-32]
}

const _LayerTypes_name = "SuperLayerInputLayerTargetLayerCompareLayerCTLayerPulvinarLayerTRNLayerPTMaintLayerPTPredLayerPTNotMaintLayerMatrixLayerSTNLayerGPLayerBGThalLayerVSGatedLayerBLALayerCeMLayerVSPatchLayerLHbLayerDrivesLayerEffortLayerUrgencyLayerUSLayerPVLayerLDTLayerVTALayerRewLayerRWPredLayerRWDaLayerTDPredLayerTDIntegLayerTDDaLayerLayerTypesN"

var _LayerTypes_index = [...]uint16{0, 10, 20, 31, 43, 50, 63, 71, 83, 94, 109, 120, 128, 135, 146, 158, 166, 174, 186, 194, 205, 216, 228, 235, 242, 250, 258, 266, 277, 286, 297, 309, 318, 329}

func (i LayerTypes) String() string {
	if i < 0 || i >= LayerTypes(len(_LayerTypes_index)-1) {
		return "LayerTypes(" + strconv.FormatInt(int64(i), 10) + ")"
	}
	return _LayerTypes_name[_LayerTypes_index[i]:_LayerTypes_index[i+1]]
}

func (i *LayerTypes) FromString(s string) error {
	for j := 0; j < len(_LayerTypes_index)-1; j++ {
		if s == _LayerTypes_name[_LayerTypes_index[j]:_LayerTypes_index[j+1]] {
			*i = LayerTypes(j)
			return nil
		}
	}
	return errors.New("String: " + s + " is not a valid option for type: LayerTypes")
}

var _LayerTypes_descMap = map[LayerTypes]string{
	0:  `Super is a superficial cortical layer (lamina 2-3-4) which does not receive direct input or targets. In more generic models, it should be used as a Hidden layer, and maps onto the Hidden type in emer.LayerType.`,
	1:  `Input is a layer that receives direct external input in its Ext inputs. Biologically, it can be a primary sensory layer, or a thalamic layer.`,
	2:  `Target is a layer that receives direct external target inputs used for driving plus-phase learning. Simple target layers are generally not used in more biological models, which instead use predictive learning via Pulvinar or related mechanisms.`,
	3:  `Compare is a layer that receives external comparison inputs, which drive statistics but do NOT drive activation or learning directly. It is rarely used in axon.`,
	4:  `CT are layer 6 corticothalamic projecting neurons, which drive &#34;top down&#34; predictions in Pulvinar layers. They maintain information over time via stronger NMDA channels and use maintained prior state information to generate predictions about current states forming on Super layers that then drive PT (5IB) bursting activity, which are the plus-phase drivers of Pulvinar activity.`,
	5:  `Pulvinar are thalamic relay cell neurons in the higher-order Pulvinar nucleus of the thalamus, and functionally isomorphic neurons in the MD thalamus, and potentially other areas. These cells alternately reflect predictions driven by CT projections, and actual outcomes driven by 5IB Burst activity from corresponding PT or Super layer neurons that provide strong driving inputs.`,
	6:  `TRNLayer is thalamic reticular nucleus layer for inhibitory competition within the thalamus.`,
	7:  `PTMaintLayer implements the subset of pyramidal tract (PT) layer 5 intrinsic bursting (5IB) deep neurons that exhibit robust, stable maintenance of activity over the duration of a goal engaged window, modulated by basal ganglia (BG) disinhibitory gating, supported by strong MaintNMDA channels and recurrent excitation. The lateral PTSelfMaint projection uses MaintG to drive GMaintRaw input that feeds into the stronger, longer MaintNMDA channels, and the ThalToPT ModulatoryG projection from BGThalamus multiplicatively modulates the strength of other inputs, such that only at the time of BG gating are these strong enough to drive sustained active maintenance. Use Act.Dend.ModGain to parameterize.`,
	8:  `PTPredLayer implements the subset of pyramidal tract (PT) layer 5 intrinsic bursting (5IB) deep neurons that combine modulatory input from PTMaintLayer sustained maintenance and CTLayer dynamic predictive learning that helps to predict state changes during the period of active goal maintenance. This layer provides the primary input to VSPatch US-timing prediction layers, and other layers that require predictive dynamic`,
	9:  `PTNotMaintLayer implements a tonically active layer that is inhibited by the PTMaintLayer, thereby providing an active representation of the *absence* of maintained PT activity, which is useful for driving appropriate actions (e.g., exploration) when not in goal-engaged mode.`,
	10: `MatrixLayer represents the matrisome medium spiny neurons (MSNs) that are the main Go / NoGo gating units in BG. These are strongly modulated by phasic dopamine: D1 = Go, D2 = NoGo.`,
	11: `STNLayer represents subthalamic nucleus neurons, with two subtypes: STNp are more strongly driven and get over bursting threshold, driving strong, rapid activation of the KCa channels, causing a long pause in firing, which creates a window during which GPe dynamics resolve Go vs. No balance. STNs are more weakly driven and thus more slowly activate KCa, resulting in a longer period of activation, during which the GPi is inhibited to prevent premature gating based only MtxGo inhibition -- gating only occurs when GPeIn signal has had a chance to integrate its MtxNo inputs.`,
	12: `GPLayer represents a globus pallidus layer in the BG, including: GPeOut, GPeIn, GPeTA (arkypallidal), and GPi. Typically just a single unit per Pool representing a given stripe.`,
	13: `BGThalLayer represents a BG gated thalamic layer, which receives BG gating in the form of an inhibitory projection from GPi. Located mainly in the Ventral thalamus: VA / VM / VL, and also parts of MD mediodorsal thalamus.`,
	14: `VSGated represents explicit coding of VS gating status: JustGated and HasGated (since last US or failed predicted US), For visualization and / or motor action signaling.`,
	15: `BLALayer represents a basolateral amygdala layer which learns to associate arbitrary stimuli (CSs) with behaviorally salient outcomes (USs)`,
	16: `CeMLayer represents a central nucleus of the amygdala layer.`,
	17: `VSPatchLayer represents a ventral striatum patch layer, which learns to represent the expected amount of dopamine reward and projects both directly with shunting inhibition to the VTA and indirectly via the LHb / RMTg to cancel phasic dopamine firing to expected rewards (i.e., reward prediction error).`,
	18: `LHbLayer represents the lateral habenula, which drives dipping in the VTA. It tracks the ContextPVLV.LHb values for visualization purposes -- updated by VTALayer.`,
	19: `DrivesLayer represents the Drives in PVLV framework. It tracks the ContextPVLV.Drives values for visualization and predictive learning purposes.`,
	20: `EffortLayer represents the Effort factor in PVLV framework. It tracks the ContextPVLV.Effort.Disc value for visualization and predictive learning purposes.`,
	21: `UrgencyLayer represents the Urgency factor in PVLV framework. It tracks the ContextPVLV.Urgency.Urge value for visualization and predictive learning purposes.`,
	22: `USLayer represents a US unconditioned stimulus layer (USpos or USneg). It tracks the ContextPVLV.USpos or USneg, for visualization and predictive learning purposes. Actual US inputs are set in PVLV.`,
	23: `PVLayer represents a PV primary value layer (PVpos or PVneg) representing the total primary value as a function of US inputs, drives, and effort. It tracks the ContextPVLV.VTA.PVpos, PVneg values for visualization and predictive learning purposes.`,
	24: `LDTLayer represents the laterodorsal tegmentum layer, which is the primary limbic ACh (acetylcholine) driver to other ACh: BG cholinergic interneurons (CIN) and nucleus basalis ACh areas. The phasic ACh release signals reward salient inputs from CS, US and US omssion, and it drives widespread disinhibition of BG gating and VTA DA firing. It receives excitation from superior colliculus which computes a temporal derivative (stimulus specific adaptation, SSA) of sensory inputs, and inhibitory input from OFC, ACC driving suppression of distracting inputs during goal-engaged states.`,
	25: `VTALayer represents the ventral tegmental area, which releases dopamine. It calls the ContextPVLV.VTA methods, and tracks resulting DA for visualization purposes.`,
	26: `RewLayer represents positive or negative reward values across 2 units, showing spiking rates for each, and Act always represents signed value.`,
	27: `RWPredLayer computes reward prediction for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the PVLV framework). Activity is computed as linear function of excitatory conductance (which can be negative -- there are no constraints). Use with RWPrjn which does simple delta-rule learning on minus-plus.`,
	28: `RWDaLayer computes a dopamine (DA) signal based on a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the PVLV framework). It computes difference between r(t) and RWPred values. r(t) is accessed directly from a Rew layer -- if no external input then no DA is computed -- critical for effective use of RW only for PV cases. RWPred prediction is also accessed directly from Rew layer to avoid any issues.`,
	29: `TDPredLayer is the temporal differences reward prediction layer. It represents estimated value V(t) in the minus phase, and computes estimated V(t+1) based on its learned weights in plus phase, using the TDPredPrjn projection type for DA modulated learning.`,
	30: `TDIntegLayer is the temporal differences reward integration layer. It represents estimated value V(t) from prior time step in the minus phase, and estimated discount * V(t+1) + r(t) in the plus phase. It gets Rew, PrevPred from Context.NeuroMod, and Special LayerVals from TDPredLayer.`,
	31: `TDDaLayer computes a dopamine (DA) signal as the temporal difference (TD) between the TDIntegLayer activations in the minus and plus phase. These are retrieved from Special LayerVals.`,
	32: ``,
}

func (i LayerTypes) Desc() string {
	if str, ok := _LayerTypes_descMap[i]; ok {
		return str
	}
	return "LayerTypes(" + strconv.FormatInt(int64(i), 10) + ")"
}
